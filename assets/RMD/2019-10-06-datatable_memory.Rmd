---
title: "Understanding `data.table` a little better"
output: md_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      message = FALSE)
```

As of late, I have used the `data.table` package to do some of my data wrangling. It has been a fun adventure (the nerd type of fun), and was made more meaningful with the renewed development of the `dtplyr` package by Hadley Wickham and co. I introduce some of the different behavior of `data.table` [here](https://tysonstanley.github.io//jekyll/update/2019/07/12/datatable/).

This post is designed to help me understand more about how `data.table` works in regards to memory and speed. This will assess the *modify-by-reference* behavior as compared to the modify-by-copy that Hadley references in Advanced R's [memory chapter](http://adv-r.had.co.nz/memory.html). 

## Packages

First, we'll use the following packages to further understand R, `data.table`, and `dplyr`.

```{r}
library(bench)      # assess speed and memory
library(data.table) # data.table for all of its stuff
library(dplyr)      # compare dt to mutate()
library(pryr)       # pry open how R works
library(lobstr)     # assess the process of R functions
```


## Example Data

We'll use the following data table for this post.

```{r}
dt <- data.table(
  grp = rbinom(1e6, 1, .5) %>% factor,
  x = rnorm(1e6),
  y = runif(1e6)
)
```

It is roughly `pryr::object_size(dt)` and has an address of `lobstr::obj_addr(dt)`. We won't be using this address later on because we'll be making copies of this data table, but note that an object has a size and an address on your computer.

```{r}
pryr::object_size(dt)
lobstr::obj_addr(dt)
```

## Memory Usage

The next few sections, I will look at the behavior of `data.table` regarding:

1. Adding a variable
2. Filtering rows
3. Summarizing data

### Adding a variable

To better understand `data.table`'s behavior regarding adding a variable, let's do four different ways of adding a variable to a data frame.

1. Use the base R way of adding a variable and filtering a variable. 
2. Use the `dplyr::mutate()` or `dplyr::filter()` function.
3. Use `data.table::copy()` to make a deep copy of the data table and then modify by reference.
4. Use the modify-by-reference inherent with `:=` in `data.table` or by using the `i` argument to filter.

Below, I grab each way and make it an R expression to be evaluated later.

```{r}
based   <- expression({dt_based$z = rnorm(1e6); dt_based})
mutated <- expression({dt_mutate <- mutate(dt_mutate, z = rnorm(1e6))})
copied  <- expression({dt_copied <- copy(dt)[, z := rnorm(1e6)]})
modify  <- expression({dt_modify[, z := rnorm(1e6)]})
```

Because both `based`, `mutated`, and `modify` will change the original `dt` data table, let's create a copy for each to use and see their corresponding addresses. Note that both change from the original `dt` object since it is fully copied now.

```{r}
dt_based <- copy(dt) %>% as.data.frame()
dt_mutate <- copy(dt) %>% as_tibble()
dt_modify <- copy(dt)
```


```{r, echo = FALSE}
data.table(` ` = c("Original", "New Base", "New Mutate", "New Modify"),
           Address = c(obj_addr(dt), 
                       obj_addr(dt_based), 
                       obj_addr(dt_mutate), 
                       obj_addr(dt_modify)))
```

> Importantly, `copy()` does something very different than just assigning to another name. `copy()` creates a whole new object while assigning to another name just creates another name pointing to the same object (until one or the other is modified). 

This idea is very important to understanding `data.table`. For example, the following lines do different things. We can see this by looking at their addresses, where `just_pointing_to_dt` is the same location and the same object. The `copied_object` is an entirely different object with a different address. (Hadley covers this in depth in Advanced R.)

```{r}
copied_object <- copy(dt)
just_pointing_to_dt <- dt
```


```{r}
data.table(` ` = c("Original", "Copied Object", "New Assigned Name"),
           Address = c(obj_addr(dt), 
                       obj_addr(copied_object), 
                       obj_addr(just_pointing_to_dt)))
```

So this means, at this point, `dt` and `justing_pointing_to_dt` are both pointing to the same object. Normally, when you then go and do something to either one, like adding a variable, R then makes a copy, as shown below.

```{r}
just_pointing_to_dt$new_var <- rnorm(1e6)

data.table(` ` = c("dt", "just_pointing_to_dt"),
           Address = c(obj_addr(dt), 
                       obj_addr(just_pointing_to_dt)))
```

Interestingly, this makes what I just did slower. Because not only what I telling R to create a new variable, but it was also making a copy of the object. However, this behavior helps us not accidentally adjust `dt` by adjusting `just_pointing_to_dt`. So now we have two separate objects, where we had just started with one before we made any adjustments.

So first, let's look at the relative speed and memory use of each of the four approaches I mentioned earlier.

```{r}
bbased <- bench::mark(eval(based), iterations = 10)
bmutat <- bench::mark(eval(mutated), iterations = 10)
bcopy  <- bench::mark(eval(copied), iterations = 10)
bmodif <- bench::mark(eval(modify), iterations = 10)
```

Results from these suggest no real differences in terms of memory and speed for base R, mutate, and modify-in-place. The copy uses more memory because it is making a full copy of the data.

```{r, echo = FALSE}
rbind(bbased,
      bmutat,
      bcopy,
      bmodif) %>% 
  select(median, mem_alloc, result)
```



### Filtering Rows

Next, we show the behavior of the four approaches for filtering rows.

```{r}
based_filter   <- expression({dt_based[dt_based$grp == 1, ]})
mutated_filter <- expression({dt_mutate <- filter(dt_mutate, grp == 1)})
copied_filter  <- expression({dt_copied <- copy(dt)[grp == 1]})
modify_filter  <- expression({dt_modify[grp == 1]})
```

Again, we will create copies to make adjustments to.

```{r}
dt_based <- copy(dt) %>% as.data.frame()
dt_mutate <- copy(dt) %>% as_tibble()
dt_modify <- copy(dt)
```

So what happens when we filter rows using these three approaches? Well, each return the same data but they don't necessarily take up the same memory or take up the same amount of time. Overall, `dplyr::filter()` and `data.table` are both faster and more efficient than the base R approach. And, as before, the copy approach takes up the most memory and speed (not shocking).

```{r}
bbased_filter <- bench::mark(eval(based_filter), iterations = 10)
bmutat_filter <- bench::mark(eval(mutated_filter), iterations = 10)
bcopy_filter  <- bench::mark(eval(copied_filter), iterations = 10)
bmodif_filter <- bench::mark(eval(modify_filter), iterations = 10)
```

```{r, echo = FALSE}
rbind(bbased_filter,
      bmutat_filter,
      bcopy_filter,
      bmodif_filter) %>% 
  select(median, mem_alloc, result)
```



### Summarizing Data

Finally, we show the behavior of the four approaches for summarizing data. First, we will simply take the mean of `x` by `grp` in each.

```{r}
based_mean   <- expression({tapply(dt_based$x, dt_based$grp, mean)})
mutated_mean <- expression({summarize(group_by(dt_mutate, grp), mean(x))})
copied_mean  <- expression({dt_copied <- copy(dt)[, mean(x), by = "grp"]})
modify_mean  <- expression({dt_modify[, mean(x), by = "grp"]})
```

Again, we will create copies to make adjustments to.

```{r}
dt_based <- copy(dt) %>% as.data.frame()
dt_mutate <- copy(dt) %>% as_tibble()
dt_modify <- copy(dt)
```

Overall, `dplyr::group_by() %>% dplyr::summarize()` is very efficient but slightly slower than `data.table`.

```{r}
bbased_mean <- bench::mark(eval(based_mean), iterations = 10)
bmutat_mean <- bench::mark(eval(mutated_mean), iterations = 10)
bcopy_mean  <- bench::mark(eval(copied_mean), iterations = 10)
bmodif_mean <- bench::mark(eval(modify_mean), iterations = 10)
```

```{r, echo = FALSE}
rbind(bbased_mean,
      bmutat_mean,
      bcopy_mean,
      bmodif_mean) %>% 
  select(median, mem_alloc, result)
```

What if we order the groups first?

```{r}
based_mean   <- expression({tapply(dt_based$x, dt_based$grp, mean)})
mutated_mean <- expression({summarize(group_by(dt_mutate, grp), mean(x))})
copied_mean  <- expression({dt_copied <- copy(dt)[, mean(x), by = "grp"]})
modify_mean  <- expression({dt_modify[, mean(x), by = "grp"]})
```

Again, we will create copies to make adjustments to.

```{r}
dt_based <- copy(dt) %>% as.data.frame() %>% .[order(.$grp), ]
dt_mutate <- copy(dt) %>% as_tibble() %>% arrange(grp)
dt_modify <- copy(dt)[order(grp)]
```

The `data.table` approach speeds up a bit while the others don't change. Again, though, `dplyr::group_by() %>% dplyr::summarize()` is very efficient compared to the others.

```{r}
bbased_mean <- bench::mark(eval(based_mean), iterations = 10)
bmutat_mean <- bench::mark(eval(mutated_mean), iterations = 10)
bcopy_mean  <- bench::mark(eval(copied_mean), iterations = 10)
bmodif_mean <- bench::mark(eval(modify_mean), iterations = 10)
```

```{r, echo = FALSE}
rbind(bbased_mean,
      bmutat_mean,
      bcopy_mean,
      bmodif_mean) %>% 
  select(median, mem_alloc, result)
```

Now let's summarize a group of variables; in this case, summarizing both `x` and `y` using loops. Since it is clear what `copy()` is doing, we won't make the comparison here.

```{r}
based_loop   <- expression({lapply(dt_based[, c("x", "y")], function(x) tapply(x, dt_based$grp, mean))})
mutated_loop <- expression({summarize_all(group_by(dt_mutate, grp), mean)})
modify_loop  <- expression({dt_modify[, lapply(.SD, mean), by = "grp"]})
```

Again, we will create copies to make adjustments to.

```{r}
dt_based <- copy(dt) %>% as.data.frame()
dt_mutate <- copy(dt) %>% as_tibble()
dt_modify <- copy(dt)
```

This shows the exact same pattern, with `dplyr::group_by() %>% dplyr::summarize()` being extremely efficient compared to the others while `data.table` is somewhat faster. 

```{r}
bbased_mean <- bench::mark(eval(based_loop), iterations = 10)
bmutat_mean <- bench::mark(eval(mutated_loop), iterations = 10)
bmodif_mean <- bench::mark(eval(modify_loop), iterations = 10)
```

```{r, echo = FALSE}
rbind(bbased_mean,
      bmutat_mean,
      bmodif_mean) %>% 
  select(median, mem_alloc, result)
```

#### Aside: Why is `dplyr` so efficient in summarizing?


```{r}
bmutat_mean$memory
bmodif_mean$memory
```



## TL;DR

In cases of adding a variable, filtering rows, and summarizing data, both `dplyr` and `data.table` perform very well. 

1. Base R, `dplyr`, and `data.table` perform similarly when adding a single variable to an already copied data set.
2. `data.table` is very efficient in filtering and both `dplyr` and `data.table` perform similarly in terms of speed.
3. `dplyr` shows great memory efficiency in summarizing, while `data.table` is generally the fastest approach.

### Package Information

Note the package information for these analyses.

```{r}
sessioninfo::package_info()
```


