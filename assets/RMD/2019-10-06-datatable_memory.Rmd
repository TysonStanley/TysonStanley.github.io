---
title: "Understanding `data.table` a little better"
output: 
  md_document: default
  html_document: default
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      message = FALSE)
```

As of late, I have used the `data.table` package to do some of my data wrangling. It has been a fun adventure (the nerd type of fun), and was made more meaningful with the renewed development of the `dtplyr` package by Hadley Wickham and co. I introduce some of the different behavior of `data.table` [here](https://tysonstanley.github.io//jekyll/update/2019/07/12/datatable/).

This post is designed to help me understand more about how `data.table` works in regards to memory and speed. This will assess the *modify-by-reference* behavior as compared to the modify-by-copy that Hadley references in Advanced R's [memory chapter](http://adv-r.had.co.nz/memory.html). 

Throughout this post, I use the terms **efficient** and **speed**. 

1. *Efficient*: refers to how much memory is used to perform a function.
2. *Speed*: refers to how quickly the function runs.

We'll be assessing these two things to understand more about `data.table` and `dplyr` (as well as base R).

## TL;DR

In cases of adding a variable, filtering rows, and summarizing data, both `dplyr` and `data.table` perform very well. 

1. Base R, `dplyr`, and `data.table` perform similarly when adding a single variable to an already copied data set.
2. `data.table` is very efficient and quick in filtering.
3. `dplyr` shows great memory efficiency in summarizing, while `data.table` is generally the fastest approach.

If you want the specifics, continue on :)

## Packages

First, we'll use the following packages to further understand R, `data.table`, and `dplyr`.

```{r}
library(bench)      # assess speed and memory
library(data.table) # data.table for all of its stuff
library(dplyr)      # compare it to data.table
library(lobstr)     # assess the process of R functions
```

And we'll set a random number seed.

```{r}
set.seed(84322)
```


## Example Data

We'll use the following data table for this post.

```{r}
dt <- data.table(
  grp = sample(c(1,2,3), size = 1e6, replace = TRUE) %>% factor,
  x = rnorm(1e6),
  y = runif(1e6)
)
dt
```

It is roughly `r round(obj_size(dt)/1e6, 1)` MB and has an address of `r obj_addr(dt)`. We won't be using this address later on because we'll be making copies of this data table, but note that an object has a size and an address on your computer.

## Comparisons

Below, I will look at the behavior of `data.table` (compared to base R and `dplyr`) regarding:

1. Adding a variable
2. Filtering rows
3. Summarizing data

Let's start with the base approaches.

### Base R

The following functions perform, in order, 1) adding a variable, 2) filtering rows, and 3) summarizing data by group using base functionality.

```{r}
base_mutate <- function(data){
  data$z <- rnorm(1e6)
  data
}
base_filter <- function(data){
  data[data$grp == 1, ]
}
base_summarize <- function(data){
  tapply(data$x, data$grp, mean)
}
```

### `dplyr`

Again, the following functions perform, in order, 1) adding a variable, 2) filtering rows, and 3) summarizing data by group using `dplyr` functions.

```{r}
dplyr_mutate <- function(data){
  mutate(data, z = rnorm(1e6))
}
dplyr_filter <- function(data){
  filter(data, grp == 1)
}
dplyr_summarize <- function(data){
  summarize(group_by(data, grp), mean(x))
}
```

### `data.table`

```{r}
dt_mutate <- function(data){
  data[, z := rnorm(1e6)]
}
dt_filter <- function(data){
  data[grp == 1]
}
dt_summarize <- function(data){
  data[, mean(x), by = "grp"]
}
```

## Copies to Benchmark

The data below are copied in order to make the benchmarking more comparable.

```{r}
df <- copy(dt) %>% as.data.frame()
tbl <- copy(dt) %>% as_tibble()
dt <- copy(dt)
```


## Benchmarking

The following benchmarking tests each situation for the three approaches.

```{r}
# Adding a variable
bench_base_m  <- bench::mark(base_mutate(df), iterations = 50)
bench_dplyr_m <- bench::mark(dplyr_mutate(tbl), iterations = 50)
bench_dt_m    <- bench::mark(dt_mutate(dt), iterations = 50)
# Filtering rows
bench_base_f  <- bench::mark(base_filter(df), iterations = 50)
bench_dplyr_f <- bench::mark(dplyr_filter(tbl), iterations = 50)
bench_dt_f    <- bench::mark(dt_filter(dt), iterations = 50)
# Summarizing by group
bench_base_s  <- bench::mark(base_summarize(df), iterations = 50)
bench_dplyr_s <- bench::mark(dplyr_summarize(tbl), iterations = 50)
bench_dt_s    <- bench::mark(dt_summarize(dt), iterations = 50)
```

## Memory Usage (Efficiency)

We will visualize the memory allocated for each approach, using `ggplot2` and `cowplot` packages.

```{r, echo = FALSE, fig.align='center', fig.width=6, fig.height=6}
library(ggplot2)
theme_set(theme_minimal() +
          theme(panel.grid.major.x = element_blank(),
                legend.position = 'none'))

p1 <- rbind(bench_base_m,
            bench_dplyr_m,
            bench_dt_m) %>% 
  mutate(expression = c("base R", "dplyr", "data.table")) %>% 
  mutate(mem_alloc = as.numeric(mem_alloc)/1024000) %>% 
  ggplot(aes(expression, mem_alloc, fill = expression)) +
    geom_bar(stat = "identity",
             alpha = .8) +
    geom_text(aes(label = round(mem_alloc, 2)),
              nudge_y = 1) +
    labs(x = "",
         y = "Memory Used (MB)",
         subtitle = "Adding a Variable") +
    scale_fill_viridis_d()

p2 <- rbind(bench_base_f,
            bench_dplyr_f,
            bench_dt_f) %>% 
  mutate(expression = c("base R", "dplyr", "data.table")) %>% 
  mutate(mem_alloc = as.numeric(mem_alloc)/1024000) %>% 
  ggplot(aes(expression, mem_alloc, fill = expression)) +
    geom_bar(stat = "identity",
             alpha = .8) +
    geom_text(aes(label = round(mem_alloc, 2)),
              nudge_y = 4) +
    labs(x = "",
         y = "Memory Used (MB)",
         subtitle = "Filtering Rows") +
    scale_fill_viridis_d()

p3 <- rbind(bench_base_s,
            bench_dplyr_s,
            bench_dt_s) %>% 
  mutate(expression = c("base R", "dplyr", "data.table")) %>% 
  mutate(mem_alloc = as.numeric(mem_alloc)/1024000) %>% 
  ggplot(aes(expression, mem_alloc, fill = expression)) +
    geom_bar(stat = "identity",
             alpha = .8) +
    geom_text(aes(label = round(mem_alloc, 2)),
              nudge_y = 2) +
    labs(x = "",
         y = "Memory Used (MB)",
         subtitle = "Summarizing by Group") +
    scale_fill_viridis_d()

library(cowplot)
plot_mem <- plot_grid(p1, p2, p3)
ggsave("2019-10-06-datatable_memory_files/fig1.png", plot = plot_mem, height = 6, width = 6)
```

Definitely some things worth noting across the approaches.

1. There are no meaningful differences when adding a variable.
2. `data.table` is the most efficient when filtering rows.
3. `dplyr` is far more efficient when summarizing by group while `data.table` was the least efficient.


## Speed

Below, we next look at the speed

```{r, echo = FALSE, fig.align='center', fig.width=6, fig.height=6}
p4 <- rbind(bench_base_m,
            bench_dplyr_m,
            bench_dt_m) %>% 
  pull(time) %>% 
  setNames(c("base R", "dplyr", "data.table")) %>% 
  bind_rows() %>% 
  tidyr::gather() %>% 
  ggplot(aes(key, value, color = key)) +
    ggbeeswarm::geom_beeswarm() +
    labs(x = "",
         y = "Time (sec)",
         subtitle = "Adding a Variable") +
    scale_color_viridis_d() +
    scale_y_log10()

p5 <- rbind(bench_base_f,
            bench_dplyr_f,
            bench_dt_f) %>% 
  pull(time) %>% 
  setNames(c("base R", "dplyr", "data.table")) %>% 
  bind_rows() %>% 
  tidyr::gather() %>% 
  ggplot(aes(key, value, color = key)) +
    ggbeeswarm::geom_beeswarm() +
    labs(x = "",
         y = "Time (sec)",
         subtitle = "Filtering Rows") +
    scale_color_viridis_d() +
    scale_y_log10()

p6 <- rbind(bench_base_s,
            bench_dplyr_s,
            bench_dt_s) %>% 
  pull(time) %>% 
  setNames(c("base R", "dplyr", "data.table")) %>% 
  bind_rows() %>% 
  tidyr::gather() %>% 
  ggplot(aes(key, value, color = key)) +
    ggbeeswarm::geom_beeswarm() +
    labs(x = "",
         y = "Time (sec)",
         subtitle = "Summarizing by Group") +
    scale_color_viridis_d() +
    scale_y_log10()

library(cowplot)
plot_speed <- plot_grid(p4, p5, p6)
ggsave("2019-10-06-datatable_memory_files/fig2.png", plot = plot_speed, height = 6, width = 6)
```


<!--
#### Aside: Why is `dplyr` so efficient in summarizing?

`dplyr` summarizes data extremely efficiently. For example, we can look at getting means of `x` for each `grp`.

```{r, eval=FALSE}
profmem::profmem(summarize(group_by(dt_mutate, grp), mean(x))) %>% 
  data.frame %>% 
  select(bytes, calls)
profmem::profmem(dt_modify[, mean(x), by = "grp"]) %>% 
  data.frame %>% 
  select(bytes, calls)
```

Interestingly, much of the memory usage is due to `gforce()`---`data.table`s way of optimizing common functions (e.g. `mean()`, `median()`). If we force `data.table` to use the `base::mean()` function instead, it actually reduces memory use.

```{r, eval = FALSE}
bench::mark(dt_modify[, base::mean(x), by = "grp"], 
            iterations = 25) %>% 
  select(median, mem_alloc)
```

Ultimately, this is something I'd love to learn more about. Be on the look out for future posts discussing this!
-->


## Session Information

Note the package information for these analyses.

```{r}
sessioninfo::session_info()
```


